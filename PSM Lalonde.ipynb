{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD7VDZBtSd3D"
      },
      "source": [
        "##Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3YmczZae7U5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/PERSONAL AGJM/PROYECTOS ML Y ESTAD./A B TESTING/Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sip-4U0pwa1y"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics as st\n",
        "\n",
        "def plot_dens(data, treatment_indicator, variables=None, xlabel='Value', title_prefix='Density by Treatment - '):\n",
        "    \"\"\"\n",
        "    Plot density distributions of multiple variables based on treatment indicator.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Pandas DataFrame, the dataset containing variables of interest.\n",
        "    - treatment_indicator: 1D NumPy array or Pandas Series, treatment indicator (0 for control, 1 for treatment).\n",
        "    - variables: List of strings, variables to plot. If None, all numeric columns in 'data' are used.\n",
        "    - xlabel: String, label for the x-axis (default: 'Value').\n",
        "    - title_prefix: String, prefix for the titles of the plots (default: 'Density Distributions by Treatment - ').\n",
        "\n",
        "    Returns:\n",
        "    - None (displays the plots).\n",
        "    \"\"\"\n",
        "\n",
        "    # If variables are not specified, use all numeric columns\n",
        "    if variables is None:\n",
        "        variables = data.select_dtypes(include=np.number).columns\n",
        "\n",
        "    num_variables = len(variables)\n",
        "    num_cols = 2\n",
        "    num_rows = int(np.ceil(num_variables / num_cols))\n",
        "\n",
        "    # If there's only one variable, adjust the layout for a single plot\n",
        "    if num_variables == 1:\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        axes = [ax]\n",
        "    else:\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    # Loop through each variable and plot density distributions\n",
        "    for i, variable in enumerate(variables):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Separate data based on treatment indicator\n",
        "        control_group = data.loc[treatment_indicator == 0, variable]\n",
        "        treatment_group = data.loc[treatment_indicator == 1, variable]\n",
        "#grey,#orange\n",
        "        # Plot density distributions using seaborn's kdeplot\n",
        "        sns.kdeplot(control_group, color='grey', label='Control', ax=ax,fill=True,alpha=0.2)\n",
        "        sns.kdeplot(treatment_group, color='red', label='Treatment', ax=ax,fill=True,alpha=0.1)\n",
        "        #label=f'Control - {variable}'\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_xlabel(xlabel,fontsize=9)\n",
        "        ax.set_ylabel('Density',fontsize=9)\n",
        "        #ax.set_title(title_prefix + variable,fontsize=13)\n",
        "        ax.set_title(variable,fontsize=14,fontweight='bold')\n",
        "        ax.legend(fontsize=9)\n",
        "\n",
        "    # Remove empty subplots for odd number of variables\n",
        "    for i in range(num_variables, len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    # Adjust layout for better spacing\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def check_balance(df,feat,flag_group):\n",
        "  if df[feat].dtype.name=='object':\n",
        "    sdf=[]\n",
        "    aux_exposed=pd.DataFrame(df[df[flag_group]==1][feat].value_counts(1)).reset_index()\n",
        "    aux_control=pd.DataFrame(df[df[flag_group]==0][feat].value_counts(1)).reset_index()\n",
        "\n",
        "    for i in aux_control['index'].unique().tolist():\n",
        "\n",
        "      prop_c=aux_control[aux_control[\"index\"]==i][feat].values[0]\n",
        "      prop_e=aux_exposed[aux_exposed[\"index\"]==i][feat].values[0]\n",
        "      res=(abs(prop_e-prop_c))/(np.sqrt((prop_e*(1-prop_e)+prop_c*(1-prop_c))*0.5))\n",
        "      sdf.append({i:res})\n",
        "\n",
        "  else:\n",
        "    # calcule the mean to both grupus:\n",
        "\n",
        "    mean_feat_control=df[df[flag_group]==0][feat].mean()\n",
        "    mean_feat_exposed=df[df[flag_group]==1][feat].mean()\n",
        "\n",
        "    # calcule the sd to both grupus:\n",
        "    st_feat_control=np.var(df[df[flag_group]==0][feat])\n",
        "    st_feat_exposed=np.var(df[df[flag_group]==1][feat])\n",
        "\n",
        "    # Calculated two elements of fracction:\n",
        "    num=mean_feat_control-mean_feat_exposed\n",
        "    den=np.sqrt((st_feat_control+st_feat_exposed)*0.5)\n",
        "    sdf=num/den\n",
        "\n",
        "  return sdf\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import pandas as pd\n",
        "\n",
        "def caliper_matching_with_covariates(treated_covariates, control_covariates, treated_propensity, control_propensity, caliper=0.005):\n",
        "    \"\"\"\n",
        "    Perform caliper matching based on propensity scores and return matched pairs with covariates.\n",
        "\n",
        "    Parameters:\n",
        "    - treated_covariates: numpy array, covariates for treated units\n",
        "    - control_covariates: numpy array, covariates for control units\n",
        "    - treated_propensity: numpy array, propensity scores for treated units\n",
        "    - control_propensity: numpy array, propensity scores for control units\n",
        "    - caliper: float, maximum allowable difference in propensity scores (default is 0.05)\n",
        "\n",
        "    Returns:\n",
        "    - matched_data: DataFrame, matched pairs with covariates\n",
        "    \"\"\"\n",
        "    # Calculate pairwise distances between treated and control propensity scores\n",
        "    distances = pairwise_distances(treated_propensity.reshape(-1, 1), control_propensity.reshape(-1, 1))\n",
        "\n",
        "    # Perform caliper matching\n",
        "    matched_pairs = []\n",
        "\n",
        "    for i in range(len(treated_propensity)):\n",
        "        min_distance = np.min(distances[i, :])\n",
        "        min_distance_index = np.argmin(distances[i, :])\n",
        "\n",
        "        if min_distance <= caliper:\n",
        "            matched_pairs.append((i, min_distance_index))\n",
        "\n",
        "    # Extract covariates for matched pairs\n",
        "    matched_treated_covariates = treated_covariates[[pair[0] for pair in matched_pairs]]\n",
        "    matched_control_covariates = control_covariates[[pair[1] for pair in matched_pairs]]\n",
        "\n",
        "    # Create a DataFrame with matched covariates\n",
        "    matched_data = pd.DataFrame(np.hstack([matched_treated_covariates, matched_control_covariates]),\n",
        "                                columns=[f't_{i}' for i in range(matched_treated_covariates.shape[1])] +\n",
        "                                        [f'c_{i}' for i in range(matched_control_covariates.shape[1])])\n",
        "\n",
        "    # Add propensity scores and treatment indicator to the DataFrame\n",
        "    matched_data['Propensity_Treated'] = treated_propensity[[pair[0] for pair in matched_pairs]]\n",
        "    matched_data['Propensity_Control'] = control_propensity[[pair[1] for pair in matched_pairs]]\n",
        "    matched_data['Treatment'] = 1  # Indicator for treated units\n",
        "    matched_data['Control'] = 0    # Indicator for control units\n",
        "\n",
        "    return matched_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fg3DdaUIjT5"
      },
      "source": [
        "## Reading a DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFyg4Z6UrZny"
      },
      "outputs": [],
      "source": [
        "columns = [\"training\",   # Treatment assignment indicator\n",
        "           \"age\",        # Age of participant\n",
        "           \"education\",  # Years of education\n",
        "           \"black\",      # Indicate whether individual is black\n",
        "           \"hispanic\",   # Indicate whether individual is hispanic\n",
        "           \"married\",    # Indicate whether individual is married\n",
        "           \"no_degree\",  # Indicate if individual has no high-school diploma\n",
        "           \"re74\",       # Real earnings in 1974, prior to study participation\n",
        "           \"re75\",       # Real earnings in 1975, prior to study participation\n",
        "           \"re78\"]       # Real earnings in 1978, after study end\n",
        "\n",
        "file_names = [\"http://www.nber.org/~rdehejia/data/nswre74_treated.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/nswre74_control.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/psid_controls.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/psid2_controls.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/psid3_controls.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/cps_controls.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/cps2_controls.txt\",\n",
        "              \"http://www.nber.org/~rdehejia/data/cps3_controls.txt\"]\n",
        "files = [pd.read_csv(file_name, delim_whitespace=True, header=None, names=columns) for file_name in file_names]\n",
        "lalonde = pd.concat(files, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3ldGUchJLKs"
      },
      "outputs": [],
      "source": [
        "print(\"DataFrame dimension:\",lalonde.shape)\n",
        "lalonde.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcjZ16bBJgOK"
      },
      "outputs": [],
      "source": [
        "print(\"Distribution of training indicator:\")\n",
        "lalonde[\"training\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_names={\"age\":\"Edad\",\"education\":\"tiempo_educacion\",\"re74\":\"Ingreso_1974\",\"re75\":\"Ingreso_1975\"}\n",
        "lalonde.rename(columns=map_names,inplace=True)"
      ],
      "metadata": {
        "id": "uX1kHU6YzaKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lalonde.sample(3)"
      ],
      "metadata": {
        "id": "hsegr1cFz85d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S6crlHyI2zo"
      },
      "source": [
        "## Check density between group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piy924wuLszB"
      },
      "outputs": [],
      "source": [
        "confounders=['Edad','tiempo_educacion','black','hispanic','married',\n",
        "            'no_degree','Ingreso_1974','Ingreso_1975']\n",
        "confounders2=['Edad','tiempo_educacion','Ingreso_1974','Ingreso_1975']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arqpka8cEQEz"
      },
      "outputs": [],
      "source": [
        "diffs=[]\n",
        "for c in confounders:\n",
        "  diffs.append({check_balance(lalonde,c,\"training\"):c})\n",
        "diffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQg83NySWQz"
      },
      "outputs": [],
      "source": [
        "plot_dens(lalonde,lalonde[\"training\"],confounders2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKaX3K1xaQhB"
      },
      "outputs": [],
      "source": [
        "lalonde[\"training\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A21TyGjKJlB"
      },
      "source": [
        "## Fit a LR to stimate propensity scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JM7gDhGJx3y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import make_scorer, roc_auc_score,precision_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Feature_4 is the outcome\n",
        "\n",
        "X = lalonde[confounders]\n",
        "treatment = lalonde[\"training\"]\n",
        "\n",
        "# Create a binary outcome variable 'y' (replace this with your actual outcome variable)\n",
        "#y = np.random.choice([0, 1], size=1000)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, treatment_train, treatment_test = train_test_split(\n",
        "    X, treatment, test_size=0.2, random_state=42,stratify=treatment)\n",
        "\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
        "X_train_smote, treatment_train_smote = smote.fit_resample(X_train, treatment_train)\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "#scaler = StandardScaler()\n",
        "#X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "#X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the classifiers with hyperparameter grids\n",
        "classifiers = {\n",
        "    'Logistic Regression': (LogisticRegression(), {\"max_iter\":[1000],\n",
        "                                                   \"class_weight\":[None,\"balanced\"]})#,\n",
        "    #'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 3,5,10]})#,\n",
        "    #'Random Forest': (RandomForestClassifier(), {'n_estimators': [10,50,100], 'max_depth': [None, 5, 10]})\n",
        "}\n",
        "\n",
        "# Fit each classifier with hyperparameter tuning, SMOTE, and cross-validation, and evaluate performance\n",
        "results = {}\n",
        "for name, (classifier, param_grid) in classifiers.items():\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring=make_scorer(roc_auc_score))\n",
        "    grid_search.fit(X_train_smote, treatment_train_smote)\n",
        "\n",
        "    # Get the best model from grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Cross-validate the model\n",
        "    cv_scores = cross_val_score(best_model, X_train_smote, treatment_train_smote, cv=5, scoring=make_scorer(roc_auc_score))\n",
        "\n",
        "    # Predict propensity scores on the testing data\n",
        "    propensity_scores = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate the performance using the Area Under the Receiver Operating Characteristic Curve (AUC score)\n",
        "    auc_score = roc_auc_score(treatment_test, propensity_scores)   # YA HICE UN RELAJO POR QUE TENGO QUE CAMBIAR TODO A PRECISION, CAMIBIEN EN GRID SEARCH PERO EN LO DEMÁS NO, REVISAR!!!!\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {'best_model': best_model, 'cv_scores': cv_scores, 'roc_auc_score': auc_score}\n",
        "\n",
        "# Identify the best-performing model\n",
        "best_model_info = max(results.items(), key=lambda x: np.mean(x[1]['cv_scores']))\n",
        "best_model_name, best_model_info = best_model_info\n",
        "\n",
        "# Print the results\n",
        "print(\"Propensity Score Estimation Results:\")\n",
        "for name, info in results.items():\n",
        "    print(f\"{name}: Best AUC Score = {info['roc_auc_score']:.4f} (Cross-validated AUC: {np.mean(info['cv_scores']):.4f}) using {info['best_model']}\")\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name} with AUC Score = {best_model_info['roc_auc_score']:.4f} (Cross-validated AUC: {np.mean(best_model_info['cv_scores']):.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX4fcPOHP_pz"
      },
      "source": [
        "## Propensity score stimation over all dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-0wycs_ywbZ"
      },
      "outputs": [],
      "source": [
        "# Add the estimation of propensity score (ps)\n",
        "best_model_ps=best_model_info[\"best_model\"]\n",
        "lalonde[\"ps\"]=best_model.predict_proba(lalonde[confounders])[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOpzi2blLLZm"
      },
      "source": [
        "## Checking Overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTxkUGbJxUnY"
      },
      "outputs": [],
      "source": [
        "# Comparing the distributions of ps for both groups\n",
        "plot_dens(lalonde,lalonde[\"training\"],variables=[\"ps\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a860-QhfQJH6"
      },
      "source": [
        "## Saving de model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQKI9BH3tI4c"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "import pickle\n",
        "with open('best_model_ps.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_model_ps, model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVSpmjkmJQps"
      },
      "source": [
        "## Caliper Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR-J-MnFSFFV"
      },
      "outputs": [],
      "source": [
        "cov=confounders+[\"re78\"]\n",
        "\n",
        "# Dataframes with covariates separated by training indicator\n",
        "treat_c=lalonde[lalonde[\"training\"]==1][cov].values\n",
        "control_c=lalonde[lalonde[\"training\"]==0][cov].values\n",
        "\n",
        "# Values of propensity score stimation\n",
        "treat_ps=lalonde[lalonde[\"training\"]==1][\"ps\"].values\n",
        "control_ps=lalonde[lalonde[\"training\"]==0][\"ps\"].values\n",
        "\n",
        "# Calculating treshold for caliper matching algorithm\n",
        "caliper=st.stdev(lalonde[\"ps\"])*0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ_qNl1ILzFN"
      },
      "outputs": [],
      "source": [
        "# Running Caliper Matching algoritm\n",
        "matched_data = caliper_matching_with_covariates(treat_c, control_c,treat_ps, control_ps, caliper=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMRa56cSVHF6"
      },
      "source": [
        "## Cleaning Matching output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq5ic-z7p8Zu"
      },
      "outputs": [],
      "source": [
        "# Variables names acording to matching\n",
        "vars_t=[x for x in matched_data.columns if x.startswith(\"t\")]\n",
        "vars_c=[x for x in matched_data.columns if x.startswith(\"c\")]\n",
        "\n",
        "# Separed the matched data  by Treatment:Training and Control: No Training\n",
        "matched_control=matched_data[vars_c+[\"Control\",\"Propensity_Control\"]]\n",
        "matched_treated=matched_data[vars_t+[\"Treatment\",\"Propensity_Treated\"]]\n",
        "\n",
        "# Cols names for the clean matched data\n",
        "#cols=confounders+[\"Training\",\"ps\"]\n",
        "\n",
        "# Dictionaries to map\n",
        "dic_control=dict(zip(matched_control.columns.tolist(),cov+[\"Training\",\"ps\"]))\n",
        "dic_treated=dict(zip(matched_treated.columns.tolist(),cov+[\"Training\",\"ps\"]))\n",
        "# Rename columns\n",
        "matched_control.rename(columns=dic_control,inplace=True)\n",
        "matched_treated.rename(columns=dic_treated,inplace=True)\n",
        "\n",
        "# Concat the matched information\n",
        "matched_final=pd.concat([matched_treated,matched_control],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCMa-hwEkmOK"
      },
      "outputs": [],
      "source": [
        "matched_final.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8nqRnLr6cG"
      },
      "outputs": [],
      "source": [
        "print(\"Rows before matching:\",lalonde.shape[0])\n",
        "print(\"Rows after matching:\",matched_final.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIAqnRvsmj6"
      },
      "source": [
        "## Check density after matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wpv9NqpMRgc"
      },
      "outputs": [],
      "source": [
        "plot_dens(matched_final,matched_final[\"Training\"],confounders2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP9G0drV8ch5"
      },
      "outputs": [],
      "source": [
        "plot_dens(matched_final,matched_final[\"Training\"],['ps'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41WsEip7zd0m"
      },
      "source": [
        "## Standardized means pre and post mathing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6shT7gV6XlrO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def love_plot(before_matching, after_matching, feature_names):\n",
        "    \"\"\"\n",
        "    Create a vertical love plot to visualize standardized mean differences before and after matching.\n",
        "\n",
        "    Parameters:\n",
        "    - before_matching: List or array of standardized mean differences before matching.\n",
        "    - after_matching: List or array of standardized mean differences after matching.\n",
        "    - feature_names: List of feature names corresponding to the differences.\n",
        "\n",
        "    Returns:\n",
        "    - None (displays the plot).\n",
        "    \"\"\"\n",
        "    num_features = len(feature_names)\n",
        "\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(4, 6))\n",
        "\n",
        "    # Plot standardized mean differences before and after matching as lines\n",
        "    ax.plot(before_matching, np.arange(num_features), marker='o', label='Before Matching', linestyle='-', color='red',alpha=0.6)\n",
        "    ax.plot(after_matching, np.arange(num_features), marker='o', label='After Matching', linestyle='-', color='blue',alpha=0.6)\n",
        "\n",
        "    # Annotate points with feature names\n",
        "    for i, txt in enumerate(feature_names):\n",
        "        ax.annotate(txt, (after_matching[i], i), textcoords=\"offset points\", xytext=(0,10), ha='left',size=10,fontweight='bold')\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_yticks(np.arange(num_features))\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_ylabel('Features')\n",
        "    ax.set_xlabel('Standardized Mean Difference')\n",
        "    ax.set_title('Standardized Mean Differences',fontweight='bold',fontsize=14)\n",
        "    ax.axvline(x=0, color='green', linestyle= 'dashed', label='Perfect balance',alpha=1)\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have lists `before_matching_diff` and `after_matching_diff` with standardized mean differences,\n",
        "# and a list `feature_names` with the corresponding feature names.\n",
        "# vertical_love_plot(before_matching_diff, after_matching_diff, feature_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fi9AQIj1QWw"
      },
      "outputs": [],
      "source": [
        "diffs_pre=[]\n",
        "for c in confounders2:\n",
        "  diffs_pre.append({check_balance(lalonde,c,\"training\"):c})\n",
        "\n",
        "diffs_post=[]\n",
        "for c in confounders2:\n",
        "  diffs_post.append({check_balance(matched_final,c,\"Training\"):c})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_leE9RN5tvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNpLf6k94OTe"
      },
      "outputs": [],
      "source": [
        "names=[list(item.values())[0] for item in diffs_pre]\n",
        "before_m=[list(item.keys())[0] for item in diffs_pre]\n",
        "after_m=[list(item.keys())[0] for item in diffs_post]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyhLWGAm7NoF"
      },
      "outputs": [],
      "source": [
        "love_plot(before_m, after_m,names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the KS statistics for each covariate"
      ],
      "metadata": {
        "id": "e17GzyieSFoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "import numpy as np\n",
        "\n",
        "def compare_distributions(sample1, sample2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Compare two distributions using the Kolmogorov-Smirnov (KS) statistic.\n",
        "\n",
        "    Parameters:\n",
        "    - sample1: First sample.\n",
        "    - sample2: Second sample.\n",
        "    - alpha: Significance level for hypothesis testing. Default is 0.05.\n",
        "\n",
        "    Returns:\n",
        "    - result: A string indicating the result of the KS test.\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform the KS test\n",
        "    ks_statistic, p_value = ks_2samp(sample1, sample2)\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value > alpha:\n",
        "        result = 'Fail to reject the null hypothesis. The distributions are similar.'\n",
        "    else:\n",
        "        result = 'Reject the null hypothesis. The distributions are significantly different.'\n",
        "\n",
        "    return result,ks_statistic\n"
      ],
      "metadata": {
        "id": "Ljs3pBvuupCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars_to_ks=[\"age\",\"education\",\"re74\",\"re75\"]\n",
        "for v in vars_to_ks:\n",
        "  control=matched_final[matched_final[\"Training\"]==0][v]\n",
        "  treat=matched_final[matched_final[\"Training\"]==1][v]\n",
        "  print(v)\n",
        "  display(compare_distributions(control,treat, alpha=0.05))\n"
      ],
      "metadata": {
        "id": "hwOnns3pvdeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applaying A/B test"
      ],
      "metadata": {
        "id": "Oti3DehtDcrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, shapiro, mannwhitneyu, norm\n",
        "\n",
        "def perform_ab_test(control_group, treatment_group, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform A/B test using the t-test method after checking for normality.\n",
        "\n",
        "    Parameters:\n",
        "    - control_group: 1D array or list, the data for the control group.\n",
        "    - treatment_group: 1D array or list, the data for the treatment group.\n",
        "    - alpha: float, significance level (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "    - result: string, indicating the result of the A/B test.\n",
        "    - p_value: float, p-value from the t-test.\n",
        "    \"\"\"\n",
        "    # Check normality using Shapiro-Wilk test\n",
        "    _, control_p_value = shapiro(control_group)\n",
        "    _, treatment_p_value = shapiro(treatment_group)\n",
        "\n",
        "    # Perform t-test only if both groups are normally distributed\n",
        "    if control_p_value > alpha and treatment_p_value > alpha:\n",
        "        t_stat, p_value = ttest_ind(control_group, treatment_group)\n",
        "\n",
        "        # Check significance\n",
        "        if p_value < alpha:\n",
        "            result = \"Statistically significant difference (Reject H0)\"\n",
        "        else:\n",
        "            result = \"No statistically significant difference (Fail to reject H0)\"\n",
        "    else:\n",
        "      # Perform Mann-Whitney U test\n",
        "      U_stat, p_value = mannwhitneyu(control_group, treatment_group, alternative='two-sided')\n",
        "      # Check significance\n",
        "      if p_value < alpha:\n",
        "\n",
        "        result = \"Statistically significant difference (Reject H0)\"\n",
        "      else:\n",
        "        result = \"No statistically significant difference (Fail to reject H0)\"\n",
        "\n",
        "    return result, p_value"
      ],
      "metadata": {
        "id": "LMkn1nXSP6lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re78_t=matched_final[matched_final[\"Training\"]==0][\"re78\"]\n",
        "re78_c=matched_final[matched_final[\"Training\"]==1][\"re78\"]"
      ],
      "metadata": {
        "id": "lJCT-M7hMSgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(re78_t)/len(re78_t))-(sum(re78_c)/len(re78_c))"
      ],
      "metadata": {
        "id": "EAanlg_2-2dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_ab_test(re78_c,re78_t)"
      ],
      "metadata": {
        "id": "6XfuDTdjMlH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re78_c_bf=lalonde[lalonde[\"training\"]==0][\"re78\"]\n",
        "re78_t_bf=lalonde[lalonde[\"training\"]==1][\"re78\"]\n",
        "perform_ab_test(re78_c_bf,re78_t_bf)"
      ],
      "metadata": {
        "id": "4Ldz13R-TklY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu, norm\n",
        "\n",
        "def perform_mannwhitneyu_test_large_sample(control_group, treatment_group, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform Mann-Whitney U test with Z approximation for large sample sizes.\n",
        "\n",
        "    Parameters:\n",
        "    - control_group: 1D array or list, the data for the control group.\n",
        "    - treatment_group: 1D array or list, the data for the treatment group.\n",
        "    - alpha: float, significance level (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "    - result: string, indicating the result of the Mann-Whitney U test.\n",
        "    - p_value: float, p-value from the test.\n",
        "    \"\"\"\n",
        "    # Perform Mann-Whitney U test\n",
        "    U_stat, p_value = mannwhitneyu(control_group, treatment_group, alternative='two-sided')\n",
        "\n",
        "    # For large samples, use Z approximation\n",
        "    n1 = len(control_group)\n",
        "    n2 = len(treatment_group)\n",
        "    Z_approx = (U_stat - (n1 * n2 / 2)) / np.sqrt((n1 * n2 * (n1 + n2 + 1)) / 12)\n",
        "\n",
        "    # Calculate p-value using the Z approximation\n",
        "    p_value_z_approx = 2 * (1 - norm.cdf(np.abs(Z_approx)))\n",
        "\n",
        "    # Check significance\n",
        "    if p_value_z_approx < alpha:\n",
        "        result = \"Statistically significant difference (Reject H0)\"\n",
        "    else:\n",
        "        result = \"No statistically significant difference (Fail to reject H0)\"\n",
        "\n",
        "    return result, p_value_z_approx\n",
        "\n"
      ],
      "metadata": {
        "id": "oKegHGFHbSD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(re78_c_bf)/len(re78_c_bf))-(sum(re78_t_bf)/len(re78_t_bf))"
      ],
      "metadata": {
        "id": "TVkzxLs_NriS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_mannwhitneyu_test_large_sample(re78_c_bf,re78_t_bf)"
      ],
      "metadata": {
        "id": "WpiQE10kb1qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_fg3DdaUIjT5",
        "6A21TyGjKJlB"
      ],
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}